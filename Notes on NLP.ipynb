{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on NLP techniques\n",
    "The notes below were taken, mostly, from an NLP class by Keith Galli, available by [this link](https://www.youtube.com/watch?v=vyOgWhwUmec), in the PyCon US Youtube Channel and research on the documentation of some of the referred libraries.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words model breaks down the sentences in the training set to word vectors, and then flattens them into one vector with unique words.<br>\n",
    "Then, each sentence will be mapped to a line inside a binary matrix. Every column of this matrix represents a word.<br>\n",
    "If $a_{11} = 1$, for example, it means that the second sentence in the set contains the second word at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\"i love the book\", \"this is a great book\", \"the fit is great\", \"i love the shoes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit_transform` method produces the vector of unique words mentioned above (obtainable via the `get_feature_names` method) and returns the corresponding binary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectors = vectorizer.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'fit', 'great', 'is', 'love', 'shoes', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 1 0 1 0]\n",
      " [1 0 1 1 0 0 0 1]\n",
      " [0 1 1 1 0 0 1 0]\n",
      " [0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix above is the representation, in the Bag of Words model, of the sentences in _train_x_.<br>\n",
    "Now, we can feed those vectors to a Support Vector Machine algorithm, so that it may receive a sentence, break it down to a binary vector for each of the words in the set and predict whether it's about books or clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category:\n",
    "    BOOKS = \"BOOKS\"\n",
    "    CLOTHING = \"CLOTHING\"\n",
    "\n",
    "train_y=[Category.BOOKS, Category.BOOKS, Category.CLOTHING, Category.CLOTHING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf_svm = svm.SVC(kernel=\"linear\")\n",
    "clf_svm.fit(vectors, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform` method does not add new words into the vector of \"features\".<br>It only transforms the sentences passed to binary vectors corresponding to the words defined when `fit_transform` was called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([\"i like the book\", 'i like the soup']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS', 'CLOTHING', 'BOOKS', 'CLOTHING'], dtype='<U8')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = vectorizer.transform([\"i like the book\", \"i hate the shoes\", \"I love this\", \"this is great\"])\n",
    "clf_svm.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with the bag of words model is it does not properly decipher meaning from the words or _n-grams_.<br>\n",
    "It only associates words to categories, so that \"love\" points to `BOOKS` and \"great\" points to `CLOTHING`, even though they are virtually unrelated.<br>\n",
    "The model also has a problem with interpreting words that are not in the training set, being incapable of estimating whether they are related to some category or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLOTHING', 'CLOTHING'], dtype='<U8')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = vectorizer.transform([\"i love the story\", \"this is a great author\"])\n",
    "clf_svm.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to converting text to numerical vectors. It attemps to assign semantic meaning to words, given a group of words.<br>\n",
    "For example, given the sentences \"I read the **book**\", \"The **book** has great **characters**\", \"The **story** in the **book** is great\", this model could relate the word \"story\" to \"book\", \"book\" to \"characters\", and, indirectly, \"story\" to \"characters\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(text) for text in train_x]\n",
    "train_x_wv = [x.vector for x in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nlp(text)` will assign a series of word embedding values to the sentence `text`, based on a relation between the words in that sentence and the sentences in the \"en_core_web_md\" training set.<br>\n",
    "For example, it probably contains sentences with both \"book\" and \"characters\", \"book\" and \"story\", etc., so that when we try to classify a sentence containing \"characters\" or \"story\", it will be related to the word \"book\", which is ultimately assigned to the category `BOOKS` according to our training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_wv = svm.SVC(kernel=\"linear\")\n",
    "clf_svm_wv.fit(train_x_wv, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS', 'CLOTHING'], dtype='<U8')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = [\"i love the book\", \"a pointy hat\"]\n",
    "test_docs = [nlp(text) for text in test_x]\n",
    "test_x_wv = [x.vector for x in test_docs]\n",
    "\n",
    "clf_svm_wv.predict(test_x_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth mentioning that the Word Vectors model assigns semantic meaning to a sentence by averaging the meaning of each word in it. This means that, for longer sentences, meaning might get lost in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLOTHING', 'BOOKS'], dtype='<U8')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = [\"A mind needs books as a sword needs a whetstone, if it is to keep its edge.\",\n",
    "          \"A mind needs books.\"]\n",
    "test_docs = [nlp(text) for text in test_x]\n",
    "test_x_wv = [x.vector for x in test_docs]\n",
    "\n",
    "clf_svm_wv.predict(test_x_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, since the model relies on the semantic meaning of each isolated word, words whose meaning varies according to context can't be relied on to define the meaning of a whole sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are patterns used to match character combinations in strings. By defining a number of boundaries to our pattern, we can verify if a string matches it or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regexp = re.compile(r\"^ab[\\S]*cd$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression above describes any string that starts with **ab**, followed by any number of characters other than a whitespace and ends with **cd**.<br>\n",
    "These expressions can be defined by combining various rules. A quick reference to those rules can be found [here](https://www.rexegg.com/regex-quickstart.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "expressions = [\"abcd\", \"abbacd\", \"cdabba\", \"ab cd\", \"xxabcdxx\"]\n",
    "matches = [bool(re.match(regexp, expression)) for expression in expressionss]\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that the `re.match` method verifies if a string, _as a whole_, matches a specific pattern. `regexp`, as it is defined above, also enforces that rule.<br>\n",
    "Let's define a new regular expression and verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False]\n"
     ]
    }
   ],
   "source": [
    "regexp = re.compile(r\"ab[\\S]*cd\")\n",
    "\n",
    "expressions = [\"xxx abcd xxx\", \"xxabbacdxx\", \"xxabcdxx\"]\n",
    "matches = [bool(re.match(regexp, expression)) for expression in expressions]\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the `re.search` method is capable of recognizing a pattern inside a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True]\n"
     ]
    }
   ],
   "source": [
    "matches = [bool(re.search(regexp, expression)) for expression in expressions]\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are useful in finding elements in text that always follow a certain formatting, such as phone numbers, document IDs, user tags (@username), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are techniques used to reduce words in a text to their essential content. Removing inflections in a word, for example, could reduce informations that are not relevant to a specific context, such as number, gender, verb tenses, etc.<br>\n",
    "For example, reducing the word \"books\" to \"book\" or \"reading\" to \"read\" could help a certain algorithm recognize those words.<br>\n",
    "<br>\n",
    "The difference between stemming and lemmatization is that, while **stemming** simply cuts out pieces of a word, **lemmatization** uses a dictionary to match inflected words to their corresponding base.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading NLTK content packages\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK stands for Natural Language Toolkit, and has a variety of methods useful in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joining', 'the', 'dots']\n"
     ]
    }
   ],
   "source": [
    "phrase = \"joining the dots\"\n",
    "words = word_tokenize(phrase)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['join', 'the', 'dot']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, WordNet is the corpus used for lemmatizing words, with NLTK establishing the mapping from a complex word to it's simplest form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joining', 'the', 'dot']\n"
     ]
    }
   ],
   "source": [
    "phrase = \"joining the dots\"\n",
    "words = word_tokenize(phrase)\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the lemmatizer above expects part of speech for each word in a sentence. By default, every word is expected to be a noun.<br>\n",
    "For \"joining the dots\", \"joining\" is treated as noun instead of a verb. To get around this, the `lemmatize` method accepts a part of speech argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['join', 'the', 'dot']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, both \"joining\" and \"dots\" are treated as verbs, since the part of speech (`pos`) specified is verb (`'v'`) for every word.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique consists of removing common words that don't play an essential role in semantics.<br>\n",
    "It can help reduce the \"meaning noise\" in long sentences while using the Word Vectors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs get sued end day goes away anyway\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Songs that we will not get sued for but at the end of the day it all goes away anyway\"\n",
    "words = word_tokenize(phrase)\n",
    "\n",
    "stripped_phrase = []\n",
    "for word in words:\n",
    "    if word not in stopwords:\n",
    "        stripped_phrase.append(word)\n",
    "\n",
    "print(\" \".join(stripped_phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob is a library that contains various utilities in processing textual data, such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = TextBlob(\"The quick brwon fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `correct` method, for example, corrects mispelled words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "phrase = phrase.correct()\n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tags` property assigns part of speech tags to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brwon', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sentiment` property captures sentiment in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=-0.85, subjectivity=1.0)\n",
      "Sentiment(polarity=0.85, subjectivity=1.0)\n"
     ]
    }
   ],
   "source": [
    "print(TextBlob(\"The evil brown fox jumps over the ugly dog\").sentiment)\n",
    "print(TextBlob(\"The beautiful brown fox jumps over the ellegant dog\").sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
